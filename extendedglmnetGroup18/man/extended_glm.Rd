% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/extended_glm.R
\name{extended_glm}
\alias{extended_glm}
\title{Extended version of glmnet}
\usage{
extended_glm(
  y,
  X,
  model = c("linear", "ridge", "lasso", "random_lasso"),
  prob = 0.7,
  len = 3,
  nfold = 5
)
}
\arguments{
\item{y}{a (non-empty) numeric vector or factor with two levels with some missing value (NA).}

\item{X}{a (non-empty) data frame or matrix where each column is a variable/predictor with some missing value (NA).For more information, see details section below.}

\item{model}{a character string specifying the type of model to fit, must be one of "linear" (default), "ridge", "lasso", or "random_lasso".}

\item{prob}{a numeric value of the proportion of data (0-1) to use for training, 0.7 (default).}

\item{len}{a numeric value of the number of tuning parameter combinations for random lasso regression, 3 (default).For more information, see details section below.}

\item{nfold}{a numeric value of the number of folds for cross validation for random lasso regression, 5 (default).For more information, see Details section below.}
}
\value{
For continuous y, it returns the root mean square error of the test data and a plotting of an actual against predicted y response while for a binary response, a list of accuracy score and confusion matrix of the test data.
}
\description{
Take as input a response variable y (binary or continuous) and a set of candidate predictors/independent variables X and fit the model the user specified even in situations where the number of predictors is large for some models;
linear, logistic, ridge, lasso, or random lasso regression.Then calculate the model performance on the test data or the entire input data, giving different metrics based on the type of response.
For continuous case, it can be assumed to be normally distributed.
}
\details{
In the algorithm of random lasso, q1 candidate variables are randomly selected in each bootstrap sample to calculate the importance measure of each predictor, and then again q2 candidate variables are randomly selected in each bootstrap sample,
where q1 and q2 are two tuning parameters that can be chosen as large as p. The function chooses q1 and q2 using cross-validation based on the number of possible q1 and q2 values, which the value len.
It selects equally distanced values of length len for q1 and q2,including 2,and p. See the reference section for more details on how random lasso works.
Furthermore, the function standardizes the numeric vectors, continuous y variable and numeric predictors. Then when calculating the root mean square error it unscales the predicted values using sample mean, and sample standard deviation of the input of y.
}
\examples{
# Fitting a linear, lasso and ridge regression with data splitting,
# 75\% for training data and 25\% for testing data

X=matrix(rnorm(100),ncol=2)
y=rexp(50)

extended_glm(y,X,model='linear',prob=0.75)
extended_glm(y,X,model='lasso',prob=0.75)
extended_glm(y,X,model='ridge',prob=0.75)

# Fitting a logistic, lasso and ridge regression with no data splitting

X_b=data.frame(x1=sample(LETTERS[1:2],50,replace=TRUE),x2=rnorm(50))
y_b=sample(c(0,1),50,replace=TRUE)

extended_glm(y_b,X_b,model='linear',prob=1)
extended_glm(y_b,X_b,model='lasso',prob=1)
extended_glm(y_b,X_b,model='ridge',prob=1)

# Fitting a random lasso regression
# 70\% for training data and 30\% for testing data
# Perform 3-fold cross validation to get the parameters
# Considers 2 possible values of q1 and q2

X=matrix(rnorm(400),ncol=4)
y=rexp(100)

X_b=data.frame(x1=sample(LETTERS[1:2],100,replace=TRUE),x2=rnorm(100),x3=runif(100))
y_b=sample(c(0,1),100,replace=TRUE)

extended_glm(y,X,model='random_lasso',prob=0.7,nfold=3,len=2)
extended_glm(y_b,X_b,model='random_lasso',prob=0.7,nfold=3,len=2)

}
\references{
Wang, S., Nan, B., Rosset, S., &amp; Zhu, J. (2011). Random lasso. The Annals of Applied Statistics, 5(1).
https://doi.org/10.1214/10-aoas377
}
